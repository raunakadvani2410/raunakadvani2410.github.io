{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data Cleaning\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    embed-resources: true\n",
        "---"
      ],
      "id": "d042cc8b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import re\n",
        "import numpy as np"
      ],
      "id": "23f8c6ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import and Merge JSON files\n",
        "\n",
        "The first step is to import all the JSON files from the 2 folders relating to DC jobs and all other jobs. We will have to import all the JSON files in each folder, merge them together, and merge the output of both folders, before converting it to a `pandas` DataFrame\n"
      ],
      "id": "be01a379"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def merge_jobs_results(directory): # merge all jsons from a folder\n",
        "    # initialize an empty list to hold the dfs\n",
        "    dfs = []\n",
        "\n",
        "    # iterate over all files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.json'):\n",
        "            # read in the json data from the file\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                json_data = json.load(f)\n",
        "\n",
        "            # extract 'jobs_results' and convert to a df\n",
        "            jobs_data = json_data.get('jobs_results', [])\n",
        "            df = pd.json_normalize(jobs_data)\n",
        "            df['key'] = filename[:-7] # add column for search key, taken from file name\n",
        "\n",
        "            # add the df to the list\n",
        "            dfs.append(df)\n",
        "\n",
        "    # concatenate into a single df\n",
        "    merged_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    return merged_df    "
      ],
      "id": "ac5296a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged = merge_jobs_results('../../data/2023-04-14-job-search/2023-04-14-job-search-location-DC/') # DC jobs"
      ],
      "id": "6a097024",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged = merged.drop('detected_extensions.commute_time', axis=1).copy() # drop last column so that both folders' files' dimensions match\n",
        "\n",
        "merged.head(3)"
      ],
      "id": "b7ddce41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged1 = merge_jobs_results('../../data/2023-04-14-job-search/2023-04-14-job-search-location-USA/') # USA jobs"
      ],
      "id": "ec704f80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "merged_df = pd.concat([merged, merged1], ignore_index=True) # concatenate DC and USA jobs\n",
        "merged_df.head(3)"
      ],
      "id": "c3c2c988",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_highlights = pd.json_normalize(merged_df['job_highlights']) # break up job highlights\n",
        "df_highlights.head(5)"
      ],
      "id": "0153705f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Data\n",
        "Now, we will clean all the necessary columns of our data, and extract relevant data\n"
      ],
      "id": "807c2bef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_items_value(dictionary): # get item value\n",
        "    if pd.isnull(dictionary): # deal with na values\n",
        "        return None\n",
        "    return dictionary['items']\n",
        "\n",
        "df_highlights = df_highlights.applymap(get_items_value) # only get the description of the item"
      ],
      "id": "d4f98257",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_highlights = df_highlights.rename(columns={0: 'qualifications', 1: 'responsibilities', 2: 'benefits'}) # rename columns\n",
        "print(df_highlights.isna().sum())"
      ],
      "id": "74a5e314",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df = pd.concat([merged_df, df_highlights], axis = 1) # merge both dataframes by column\n",
        "master_df = master_df.drop(['job_highlights', 'related_links', 'job_id', 'detected_extensions.work_from_home', 'detected_extensions.posted_at'], axis = 1) # drop unneeded columns\n",
        "\n",
        "for i in master_df: # check for nas\n",
        "    print(i, master_df[i].isna().sum())"
      ],
      "id": "284eb00c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#rename columns\n",
        "master_df = master_df.rename(columns={'detected_extensions.schedule_type': 'schedule_type', 'detected_extensions.salary': 'salary', 'key': 'query'})\n",
        "\n",
        "master_df = master_df.reset_index(drop=True)\n",
        "\n",
        "master_df.head(5)"
      ],
      "id": "680acecf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['query'] = master_df['query'].str.replace('-', ' ') # replace hyphens with spaces\n",
        "master_df.loc[master_df['query'] == 'block chain', 'query'] = 'blockchain' # remove space from blockchain"
      ],
      "id": "2a72733d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['via'] = master_df['via'].str.slice(4) # remove 'via' from the start of the string\n",
        "\n",
        "master_df = master_df.join(master_df['extensions'].apply(lambda x: pd.Series(x)).add_prefix('ext_'))\n",
        "master_df = master_df.drop('extensions', axis=1)"
      ],
      "id": "497fc391",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df.head(5)"
      ],
      "id": "0c4bbf1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['schedule_type'].unique()"
      ],
      "id": "a0dd5d32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# loop through each row in the dataframe to delete 'days ago' 'hours ago'\n",
        "for i, row in master_df.iterrows():\n",
        "    # check if 'ago' is present in ext_0\n",
        "    if 'ago' in str(row['ext_0']):\n",
        "        # if it is, replace the 'ago' value with the corresponding value from ext_1\n",
        "        master_df.at[i, 'ext_0'] = row['ext_1']\n",
        "        master_df.at[i, 'ext_1'] = np.nan\n",
        "    else:\n",
        "        # if it isn't, keep the row as it is\n",
        "        pass"
      ],
      "id": "8bc46e99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df.isna().sum()"
      ],
      "id": "a17b5ee1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df = master_df.drop(['ext_0', 'ext_1', 'ext_2', 'ext_3', 'ext_4', 'ext_5', 'ext_6', 'ext_7'], axis = 1)"
      ],
      "id": "2ea5be1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i, row in master_df.iterrows(): # go through responsibilities and benefits column to extract salary\n",
        "    if pd.isna(row['salary']):\n",
        "        if '$' in str(row['responsibilities']):\n",
        "            master_df.at[i, 'salary'] = row['responsibilities']\n",
        "        elif 'hour' in str(row['responsibilities']):\n",
        "            master_df.at[i, 'salary'] = row['responsibilities']\n",
        "        elif 'year' in str(row['responsibilities']):\n",
        "            master_df.at[i, 'salary'] = row['responsibilities']\n",
        "        elif '$' in str(row['benefits']):\n",
        "            master_df.at[i, 'salary'] = row['benefits']\n",
        "        elif 'hour' in str(row['benefits']):\n",
        "            master_df.at[i, 'salary'] = row['benefits']\n",
        "        elif 'year' in str(row['benefits']):\n",
        "            master_df.at[i, 'salary'] = row['benefits']"
      ],
      "id": "0f48f906",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['degree'] = np.nan"
      ],
      "id": "b540d6fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i, row in master_df.iterrows(): # go through responsibilities and benefits column to extract salary\n",
        "    if pd.isna(row['degree']):\n",
        "        if \"PhD\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"PhD\"\n",
        "        elif \"Ph.D.\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"PhD\"\n",
        "        elif \"Doctorate\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"PhD\"\n",
        "        elif \"Master\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"Master's\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"Masters\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"Msc\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"M.A.\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"MA\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"MS\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"Advanced\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"M.S.\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Master's\"\n",
        "        elif \"Bachelor's\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\"\n",
        "        elif \"BS\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\"\n",
        "        elif \"B.S.\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\"\n",
        "        elif \"BA\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\"\n",
        "        elif \"B.A.\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\"\n",
        "        elif \"Bachelors\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\"\n",
        "        elif \"Bachelor\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\"\n",
        "        elif \"Undergraduate\" in str(row['qualifications']):\n",
        "            master_df.at[i, 'degree'] = \"Bachelor's\""
      ],
      "id": "0952d320",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['salary'] = master_df['salary'].apply(lambda x: str(x) if isinstance(x, list) else x) # convert salary to string "
      ],
      "id": "9ec62b08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_salary(salary):\n",
        "    # if salary is a list, extract the salary range from the appropriate string\n",
        "    if isinstance(salary, list):\n",
        "        for s in salary:\n",
        "            if \"salary range\" in s.lower():\n",
        "                salary = re.findall(r\"\\$\\d+(?:,\\d+)*(?:\\.\\d+)?\", s)\n",
        "                salary = \" - \".join(salary)\n",
        "                break\n",
        "    # if salary is a string, extract the salary range\n",
        "    elif isinstance(salary, str):\n",
        "        salary = re.findall(r\"\\$\\d+(?:,\\d+)*(?:\\.\\d+)?\", salary)\n",
        "        salary = \" - \".join(salary)\n",
        "    else:\n",
        "        salary = np.nan\n",
        "    return salary\n",
        "\n",
        "def extract_time_period(salary):\n",
        "    if isinstance(salary, str):\n",
        "        if \"hour\" in salary:\n",
        "            time_period = \"an hour\"\n",
        "        elif \"month\" in salary:\n",
        "            time_period = \"a month\"\n",
        "        elif \"year\" in salary:\n",
        "            time_period = \"a year\"\n",
        "        else:\n",
        "            time_period = np.nan\n",
        "    else:\n",
        "        time_period = np.nan\n",
        "    return time_period\n",
        "\n",
        "master_df[\"salary_cleaned\"] = master_df[\"salary\"].apply(clean_salary)\n",
        "master_df[\"time_period\"] = master_df[\"salary_cleaned\"].apply(extract_time_period)"
      ],
      "id": "e99c750d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# iterate over every row in the dataframe\n",
        "for index, row in master_df.iterrows():\n",
        "    # check if the 'salary_cleaned' value contains '$'\n",
        "    if '$' in str(row['salary_cleaned']):\n",
        "        # assign the entire value to the 'salary' column\n",
        "        master_df.at[index, 'salary'] = row['salary_cleaned'] #####"
      ],
      "id": "528df2ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# remove list values from salary\n",
        "# create a boolean mask to filter out missing values \n",
        "not_null_mask = pd.notnull(master_df['salary']) \n",
        "\n",
        "# find the rows where 'salary' contains '['\n",
        "rows_with_brackets = master_df[not_null_mask & master_df['salary'].str.contains('\\\\[')]\n",
        "\n",
        "# set the 'salary' column to NaN for the rows with brackets\n",
        "master_df.loc[rows_with_brackets.index, 'salary'] = np.nan"
      ],
      "id": "ded48a5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['salary'] = master_df['salary'].str.replace('â€“', '-')"
      ],
      "id": "b52aad2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create a boolean mask to filter for rows containing 'hour'\n",
        "mask = master_df['salary'].str.contains('hour', na=False)\n",
        "\n",
        "# select the rows where 'salary' contains 'hour'\n",
        "rows_with_hourly_wage = master_df[mask]\n",
        "\n",
        "# convert hourly wage to annual salary\n",
        "for idx, row in rows_with_hourly_wage.iterrows():\n",
        "    salary_str = row['salary']\n",
        "    if salary_str.count('-') == 1:\n",
        "        # handle case where salary range is given\n",
        "        salary_range = salary_str.split(' ')[0]\n",
        "        salary_range = salary_range.replace('\\u2011', '-')\n",
        "        start, end = map(float, salary_range.split('-'))\n",
        "        avg_salary = (start + end) / 2.0\n",
        "        annual_salary = avg_salary * 2080\n",
        "        master_df.loc[idx, 'salary'] = '${:,.2f}'.format(annual_salary)\n",
        "    elif salary_str.count('-') == 0:\n",
        "        # handle case where single hourly wage is given\n",
        "        hourly_wage_str = salary_str.split(' ')[0]\n",
        "        hourly_wage = float(hourly_wage_str)\n",
        "        annual_salary = hourly_wage * 2080\n",
        "        master_df.loc[idx, 'salary'] = '${:,.2f}'.format(annual_salary)"
      ],
      "id": "d0da85ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['salary'] = master_df['salary'].str.replace(' a year', '').str.strip()\n",
        "master_df['salary'] = master_df['salary'].str.replace('$', '').str.strip()\n",
        "master_df.loc[636, 'salary'] = 136284\n",
        "master_df.loc[761, 'salary'] = 22070\n",
        "master_df.loc[681, 'salary'] = 121000\n",
        "master_df.loc[746, 'salary'] = 206000"
      ],
      "id": "850faffb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_nas = master_df['salary'].isna().sum()\n",
        "\n",
        "print(f\"Number of NaN values in 'salary' column: {num_nas}\")"
      ],
      "id": "7f8b41e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define a function to convert salary ranges to their average values\n",
        "master_df['salary'] = master_df['salary'].astype(str)\n",
        "\n",
        "def parse_salary_range(s):\n",
        "    if '-' in s:\n",
        "        s = s.replace('$', '').replace(',', '').replace('K', '000').replace('k', '000')\n",
        "        parts = s.split('-')\n",
        "        parts = [float(p) for p in parts if not pd.isnull(p)]\n",
        "        avg = sum(parts) / len(parts)\n",
        "        return avg\n",
        "    else:\n",
        "        return s\n",
        "\n",
        "# apply the function to the 'salary' column of master_df\n",
        "master_df['salary'] = master_df['salary'].apply(parse_salary_range)"
      ],
      "id": "1271dc73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['salary'] = master_df['salary'].astype(str)  # convert all data points to string type\n",
        "master_df['salary'] = master_df['salary'].str.replace(',', '')  # remove commas\n",
        "master_df['salary'] = master_df['salary'].str.replace('\\.\\d+', '')  # remove decimals and everything after them\n",
        "master_df['salary'] = pd.to_numeric(master_df['salary'], errors='coerce')\n",
        "master_df['salary'].describe()"
      ],
      "id": "8499f45f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['qualifications'] = master_df['qualifications'].astype(str)\n",
        "\n",
        "def extract_experience(text):\n",
        "    match = re.search(r'(\\d[\\d+-]*\\s*(?:year|yr|yrs|years))', text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "master_df['experience'] = master_df['qualifications'].apply(extract_experience)"
      ],
      "id": "0b1814fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['experience'] = master_df['experience'].apply(lambda x: str(x))\n",
        "\n",
        "def clean_experience(exp):\n",
        "    if isinstance(exp, str):\n",
        "        # extract digits from the experience string\n",
        "        digits = re.findall('\\d+', exp)\n",
        "        if len(digits) == 2:\n",
        "            # calculate the average of two numbers if there are two digits\n",
        "            avg_exp = (int(digits[0]) + int(digits[1])) / 2\n",
        "        elif len(digits) == 1:\n",
        "            # take the single digit if there is only one digit\n",
        "            avg_exp = int(digits[0])\n",
        "        else:\n",
        "            # return None if there are no digits\n",
        "            return None\n",
        "        return avg_exp\n",
        "    else:\n",
        "        # return None if the input is not a string\n",
        "        return None\n",
        "\n",
        "master_df['experience'] = master_df['experience'].apply(clean_experience)"
      ],
      "id": "5f9c4272",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df = master_df.drop(['salary_cleaned', 'time_period'], axis = 1)"
      ],
      "id": "59810c4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df.isna().sum()"
      ],
      "id": "5e43cb32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "min_count = master_df.count().min()\n",
        "print(\"Number of rows with entries for every column:\", min_count)"
      ],
      "id": "33e7a0ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['degree'] = master_df['degree'].astype('category')"
      ],
      "id": "32f1e1d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df.dtypes"
      ],
      "id": "9c689c0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['location'] = master_df['location'].str.strip() # delete extra spaces\n",
        "master_df['remote'] = master_df['location'].apply(lambda x: True if x in ['Anywhere', 'United States'] else False) # make 'remote' dummy variable"
      ],
      "id": "98420fbf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df.loc[[26, 58, 73, 77, 127, 230, 271, 322, 463, 472, 651, 731, 763, 768, 781], 'salary'] = np.nan # delete bonuses that were detected as salaries\n",
        "\n",
        "master_df.loc[[48, 110, 193, 264, 614, 654], 'salary'] *= 2080 # convert unconverted hourly salaries\n",
        "\n",
        "master_df.loc[[108, 143, 151, 231, 249, 340, 350, 357, 396, 426, 525, 542, 557, 627, 737, 758, 808], 'salary'] *= 1000 # salaries read as 240 instead of 240000\n",
        "\n",
        "master_df.loc[[251, 696], 'salary'] *= 12 # convert unconverted monthly salaries"
      ],
      "id": "e0a3801f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df['query'] = master_df['query'].apply(lambda x: ' '.join(word if word == 'and' else word.title() for word in x.split()))"
      ],
      "id": "0e0220d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_location(location):\n",
        "    # find the index of the '(' character\n",
        "    index = location.find('(')\n",
        "    if index != -1:\n",
        "        # if '(' is found, remove it and everything that comes after it\n",
        "        location = location[:index].strip()\n",
        "    # remove extra spaces at the start and end of the string\n",
        "    return location.strip()\n",
        "\n",
        "master_df['location'] = master_df['location'].apply(clean_location)"
      ],
      "id": "9ac83772",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define a function to extract city and state information from the location string\n",
        "def get_city_state(location):\n",
        "    # split the location string by ','\n",
        "    parts = location.split(',')\n",
        "    if len(parts) == 2:\n",
        "        # if the location has two parts, assume the first is the city and the second is the state\n",
        "        city = parts[0].strip()\n",
        "        state = parts[1].strip()\n",
        "        return city, state\n",
        "    elif len(parts) == 1:\n",
        "        # if the location has only one part, assume it is the state\n",
        "        state = parts[0].strip()\n",
        "        if location == 'Anywhere' or location == 'United States':\n",
        "            return '', ''\n",
        "        else:\n",
        "            return '', state\n",
        "    else:\n",
        "        # if the location has more than two parts, assume it is not a valid city, state format\n",
        "        return '', ''\n",
        "\n",
        "# apply the function to the location column\n",
        "master_df[['city', 'state']] = master_df['location'].apply(lambda x: pd.Series(get_city_state(x)))\n",
        "\n",
        "# handle special case for state abbreviations\n",
        "master_df['state'] = master_df['state'].apply(lambda x: 'TX' if x == 'Texas' else x)"
      ],
      "id": "dc19da2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# define a dictionary mapping state abbreviations to their full form\n",
        "state_abbreviations = {\n",
        "    'AL': 'Alabama', \n",
        "    'AK': 'Alaska', \n",
        "    'AZ': 'Arizona', \n",
        "    'AR': 'Arkansas', \n",
        "    'CA': 'California', \n",
        "    'CO': 'Colorado', \n",
        "    'CT': 'Connecticut', \n",
        "    'DE': 'Delaware', \n",
        "    'DC': 'District of Columbia', \n",
        "    'FL': 'Florida', \n",
        "    'GA': 'Georgia', \n",
        "    'HI': 'Hawaii', \n",
        "    'ID': 'Idaho', \n",
        "    'IL': 'Illinois', \n",
        "    'IN': 'Indiana', \n",
        "    'IA': 'Iowa', \n",
        "    'KS': 'Kansas', \n",
        "    'KY': 'Kentucky', \n",
        "    'LA': 'Louisiana', \n",
        "    'ME': 'Maine', \n",
        "    'MD': 'Maryland', \n",
        "    'MA': 'Massachusetts', \n",
        "    'MI': 'Michigan', \n",
        "    'MN': 'Minnesota', \n",
        "    'MS': 'Mississippi', \n",
        "    'MO': 'Missouri', \n",
        "    'MT': 'Montana', \n",
        "    'NE': 'Nebraska', \n",
        "    'NV': 'Nevada', \n",
        "    'NH': 'New Hampshire', \n",
        "    'NJ': 'New Jersey', \n",
        "    'NM': 'New Mexico', \n",
        "    'NY': 'New York', \n",
        "    'NC': 'North Carolina', \n",
        "    'ND': 'North Dakota', \n",
        "    'OH': 'Ohio', \n",
        "    'OK': 'Oklahoma', \n",
        "    'OR': 'Oregon', \n",
        "    'PA': 'Pennsylvania', \n",
        "    'RI': 'Rhode Island', \n",
        "    'SC': 'South Carolina', \n",
        "    'SD': 'South Dakota', \n",
        "    'TN': 'Tennessee', \n",
        "    'TX': 'Texas', \n",
        "    'UT': 'Utah', \n",
        "    'VT': 'Vermont', \n",
        "    'VA': 'Virginia', \n",
        "    'WA': 'Washington', \n",
        "    'WV': 'West Virginia', \n",
        "    'WI': 'Wisconsin', \n",
        "    'WY': 'Wyoming'\n",
        "}\n",
        "\n",
        "# apply the mapping to the 'state' column\n",
        "master_df['state'] = master_df['state'].apply(lambda x: state_abbreviations.get(x, x))"
      ],
      "id": "c1ec482f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Cleaned CSV\n"
      ],
      "id": "b923d9eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "master_df.to_csv('job_data.csv')"
      ],
      "id": "58b4897c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}