[
  {
    "objectID": "vis_py.html",
    "href": "vis_py.html",
    "title": "Python",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport warnings\nimport altair as alt\nwarnings.filterwarnings('ignore')\nimport os\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport networkx as nx\nimport itertools\nimport random\nimport folium\n\n\ndf = pd.read_csv('../../data/job_data.csv')\ndf['query'] = df['query'].apply(lambda x: ' '.join(word if word == 'and' else word.title() for word in x.split()))\n\n\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      title\n      company_name\n      location\n      via\n      description\n      schedule_type\n      salary\n      query\n      qualifications\n      responsibilities\n      benefits\n      degree\n      experience\n      remote\n    \n  \n  \n    \n      0\n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      Full-time\n      NaN\n      Blockchain\n      ['2-3 years of Software Development experience...\n      ['Design, maintain and deploy smart contracts ...\n      NaN\n      NaN\n      2.5\n      True\n    \n    \n      1\n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      Greenhouse\n      We are seeking a highly motivated and skilled ...\n      Full-time\n      180000.0\n      Blockchain\n      [\"Bachelor's or Master's degree in Computer Sc...\n      ['As a Blockchain Engineer, you will be respon...\n      ['(NYC only) Pursuant to Section 8-102 of titl...\n      Master's\n      NaN\n      False\n    \n    \n      2\n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      Contractor\n      NaN\n      Blockchain\n      ['3+ years of experience in blockchain, crypto...\n      ['Our expert technical team will provide the s...\n      NaN\n      NaN\n      3.0\n      True\n    \n    \n      3\n      3\n      Python based - Blockchain developer to join ex...\n      Upwork\n      Anywhere\n      Upwork\n      Need someone to join our existing team to spee...\n      Contractor\n      41600.0\n      Blockchain\n      ['Candidates must be willing to sign, non-disc...\n      ['Will discuss details with the selected candi...\n      NaN\n      NaN\n      NaN\n      True\n    \n    \n      4\n      4\n      Blockchain DevOps Engineer (Remote)\n      Telnyx\n      United States\n      Startup Jobs\n      About Telnyx\\n\\nAt Telnyx, we’re architecting ...\n      Full-time\n      NaN\n      Blockchain\n      ['You are a highly motivated and experienced B...\n      ['To build a best-in-class Filecoin (FIL) Mini...\n      NaN\n      Bachelor's\n      NaN\n      True\n    \n    \n      5\n      5\n      Ethereum Developer Remote US (Blockchain-DeFi)\n      Turnblock.io\n      Anywhere\n      ZipRecruiter\n      Our client is on crypto’s cutting edge technol...\n      Full-time\n      150000.0\n      Blockchain\n      ['5+ years of development experience', 'Experi...\n      ['As a member of an agile engineering team, yo...\n      ['Great compensation package']\n      NaN\n      5.0\n      True\n    \n    \n      6\n      6\n      Social Media Marketing (Blockchain/Crypto)\n      Bitquery\n      Anywhere\n      AngelList\n      • Maintain a social media calendar\\n• Build Bi...\n      Full-time\n      NaN\n      Blockchain\n      ['English Proficiency (Written and Speaking) w...\n      ['Put our Bitquery updates and educational con...\n      ['Opportunity to work & collaborate with a tru...\n      NaN\n      NaN\n      True\n    \n    \n      7\n      7\n      Senior Software Engineer - Blockchain Network ...\n      Jobot\n      Anywhere\n      Dice\n      competitive salaries, stock options, company p...\n      Full-time\n      175000.0\n      Blockchain\n      ['At least 5 years of experience with designin...\n      ['Rotating breakfast menu served daily', 'Dinn...\n      ['Salary: $150,000 - $200,000 per year', 'Our ...\n      NaN\n      5.0\n      True\n    \n    \n      8\n      8\n      Blockchain Developer\n      Atechstar\n      United States\n      OPTnation\n      Requirements: Strong software development back...\n      Full-time\n      141000.0\n      Blockchain\n      ['Requirements: Strong software development ba...\n      NaN\n      NaN\n      NaN\n      NaN\n      True\n    \n    \n      9\n      9\n      DeFi Blockchain Co-Founder\n      Cryptops Exchange\n      Anywhere\n      LinkedIn\n      Cryptops is currently looking for a Co-Founder...\n      Full-time\n      NaN\n      Blockchain\n      ['Can personally invest or immediately support...\n      ['Other decentralized derivatives exchanges re...\n      NaN\n      NaN\n      NaN\n      True\n    \n  \n\n\n\n\n\n\n\n# filter out 'Anywhere' and 'United States' locations\ndf_filtered = df.loc[~df['location'].isin(['Anywhere', 'United States'])]\n\n# calculate the average salary for each location\ndf_avg_salary = df_filtered.groupby('location')['salary'].mean().reset_index()\n\n# sort the locations by their frequency and select the top 5\ndf_top_locations = df_filtered['location'].value_counts().head(5).reset_index()\ndf_top_locations = df_avg_salary.loc[df_avg_salary['location'].isin(df_top_locations['index'])]\n\ndf_top_locations = df_top_locations.sort_values('salary', ascending=False)\n\n# create the bar plot\nbar = go.Bar(x=df_top_locations['location'], y=df_top_locations['salary'], name = '', marker=dict(color='crimson'),\n             hovertemplate='<b>City: </b>%{x}<br><b>Average Salary: </b>$%{y:,.2f}')\n\n# set the plot layout\nlayout = go.Layout(\n                   xaxis_title='Location',\n                   yaxis_title='Average Annual Salary (USD)',\n                   yaxis=dict(tickformat='$,.0f'),\n                   template='plotly_dark')\n\n# create the figure and show the plot\nfig = go.Figure(data=[bar], layout=layout)\nfig.show()\n\n\nfig.write_html(\"../../website/plots/plot-18.html\")\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Filter out rows with location \"Anywhere\" or \"United States\"\ndf_filtered = df[~df['location'].isin(['Anywhere', 'United States'])]\n\n# Group by location and count the number of jobs\ngrouped_df = df_filtered.groupby('location')['title'].count().reset_index()\n\n# Get the top 5 locations by job count\ntop_5 = grouped_df.nlargest(5, 'title')\n\n# Create a bar plot using Plotly\nfig = go.Figure(data=[go.Bar(x=top_5['location'], y=top_5['title'],\n                             hovertemplate='<b>City: </b> %{x}<br><b>Number of Jobs: </b>%{y}',\n                             name='',\n                             marker=dict(color='crimson'))])\nfig.update_layout(\n                  xaxis_title='Location', yaxis_title='Count of Jobs',\n                  template='plotly_dark')\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-1.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Group by via and count the number of jobs\ngrouped_df = df.groupby('via')['title'].count().reset_index()\n\n# Get the top 5 via values by job count\ntop_5 = grouped_df.nlargest(5, 'title')\n\n# Create a bar plot using Plotly\nfig = go.Figure(data=[go.Bar(x=top_5['via'], y=top_5['title'],\n                             hovertemplate='<b>Posting Site: </b>%{x}<br><b>Number of Jobs: </b>%{y}',\n                             name = '',\n                             marker=dict(color='crimson'))])\nfig.update_layout(title='',\n                  xaxis_title='Job Domain', yaxis_title='Count of Jobs',\n                  template='plotly_dark')\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-2.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# group by title and count the number of jobs\ngrouped_df = df_filtered.groupby('title')['location'].count().reset_index()\n\n# get the top 5 titles by job count\ntop_5 = grouped_df.nlargest(5, 'location')\n\n# create a bar plot using Plotly\nfig = go.Figure(data=[go.Bar(x=top_5['title'], y=top_5['location'],\n                             hovertemplate='<b>Title: </b>%{x}<b><br>Number of Jobs: </b>%{y}',\n                             name = '',\n                             marker=dict(color='crimson'))])\nfig.update_layout(\n                  xaxis_title='Job Title', yaxis_title='Count of Jobs',\n                  template='plotly_dark')\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-3.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# get the top 5 most occurring job titles\ntop_5_titles = df['title'].value_counts().nlargest(5).index.tolist()\n\n# filter the dataframe to only include the top 5 titles\ndf_filtered = df[df['title'].isin(top_5_titles)]\n\n# get the average salary for each title\ngrouped_df = df_filtered.groupby('title')['salary'].mean().reset_index()\ngrouped_df = grouped_df.sort_values('salary', ascending=False)\n# create a bar plot using Plotly\nfig = go.Figure(data=[go.Bar(x=grouped_df['title'], y=grouped_df['salary'],\n                             hovertemplate='<b>Title: </b>%{x}<b><br>Average Salary: </b>%{y:$,.2f}',\n                             name='',\n                             marker=dict(color='crimson'))])\nfig.update_layout(\n                  xaxis_title='Job Title', yaxis_title='Average Annual Salary (USD)',\n                  template='plotly_dark')\nfig.update_layout(\n                  yaxis=dict(tickformat='$,.0f'),)\n\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-7.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n# create a co-occurrence matrix between job titles and queries\ncooc_mat = pd.crosstab(df['title'], df['query'])\n\n# downsample to only keep the top 10 most common queries\ntop_queries = cooc_mat.sum(axis=0).sort_values(ascending=False)[:8].index\ncooc_mat = cooc_mat[top_queries]\n\n# downsample to only keep the top 10 most common job titles\ntop_titles = cooc_mat.sum(axis=1).sort_values(ascending=False)[:8].index\ncooc_mat = cooc_mat.loc[top_titles]\n\n# create a NetworkX graph object\nG = nx.Graph()\n\n# add nodes for each job title and query\nG.add_nodes_from(top_titles, bipartite=0)\nG.add_nodes_from(top_queries, bipartite=1)\n\n# add edges between job titles and queries based on the co-occurrence matrix\nedges = [(title, query, cooc_mat.loc[title, query]) for title in top_titles for query in top_queries]\nG.add_weighted_edges_from(edges)\n\n# project the graph onto the job titles nodes to create a job title co-occurrence network\nP = nx.bipartite.weighted_projected_graph(G, top_titles)\n\n# create a layout for the nodes in the job title co-occurrence network\npos = nx.spring_layout(P, seed=42)\n\n# set the plot style and settings\nplt.style.use('dark_background')\n#plt.rcParams['text.color'] = 'white'\nplt.rcParams['figure.facecolor'] = '#121212'\n\n# draw the job title co-occurrence network\nplt.figure(figsize=(7, 6))\nnx.draw_networkx_nodes(P, pos, node_color='crimson', node_size=1000)\nnx.draw_networkx_edges(P, pos, edge_color='white', alpha=0.5, width=[d['weight']/20 for (u,v,d) in P.edges(data=True)])\nnx.draw_networkx_labels(P, pos, font_size=12, font_family='arial', font_color='white')\nplt.axis('off')\n#plt.show(block = False)\n\n\n\n\nplt.savefig('../../website/plots/plot-17.png')\n\n\n\n\n\nimport mpld3\nfrom mpld3 import plugins\n\n\n# group the data by query and degree, and count the number of jobs\ndf_grouped = df.groupby(['query', 'degree']).size().reset_index(name='count')\n\n# define the colors for each degree\ncolors = {'Bachelor\\'s': 'rgb(31, 119, 180)', 'Master\\'s': 'rgb(255, 127, 14)', 'PhD': 'rgb(44, 160, 44)'}\n\n# create a trace for each degree\ntraces = []\nfor degree in df_grouped['degree'].unique():\n    trace = go.Bar(\n        x=df_grouped[df_grouped['degree'] == degree]['query'],\n        y=df_grouped[df_grouped['degree'] == degree]['count'],\n        name=degree,\n        marker=dict(color=colors[degree]),\n        hovertemplate='<b>Query: </b>%{x} '+ '<b><br>Number of Jobs: </b>%{y}'\n    )\n    traces.append(trace)\n\n# set the layout for the chart\nlayout = go.Layout(\n    barmode='group',\n    template=\"plotly_dark\",\n    xaxis=dict(title='Query'),\n    yaxis=dict(title='Count of Jobs')\n)\n\n# create the figure object and plot it\nfig = go.Figure(data=traces, layout=layout)\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-4.html\")\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n# create a histogram using Plotly\nfig = go.Figure(data=[go.Histogram(x=df['salary'], nbinsx=15, marker=dict(color='crimson'),\n                                   hovertemplate='<b>Annual Salary Range:</b> %{x}<b><br>Number of Jobs: </b>%{y}', name = '')])\n\n# cpdate the layout of the figure\nfig.update_layout(\n                  xaxis_title='Annual Salary (USD)', yaxis_title='Count of Jobs',\n                  template='plotly_dark',\n                  xaxis=dict(tickformat='$,.0f')\n                  )\n\n# show the figure\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-5.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n# create a histogram using Plotly\nfig = go.Figure(data=[go.Histogram(x=df['experience'], nbinsx=14, marker=dict(color='crimson'),\n                                   hovertemplate='<b>Experience Range: </b>%{x} years<br><b>Number of Jobs: </b>%{y}', name = '')])\n\n# update the layout of the figure\nfig.update_layout(\n                  xaxis_title='Experience Required (Years)', yaxis_title='Count of Jobs',\n                  template='plotly_dark')\n\n# show the figure\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-6.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\ndf['via'].value_counts()\n\nLinkedIn                    66\nZipRecruiter                60\nUpwork                      56\nAngelList                   36\nLever                       23\n                            ..\nTeradyne Companies           1\nStaples Jobs                 1\nAlexandria, VA - Geebo       1\nParsons Corporation Jobs     1\nWDHN Jobs                    1\nName: via, Length: 219, dtype: int64\n\n\n\n\n\n# group by state and calculate average salary\nstate_salaries = df.groupby('state').mean().reset_index()\n\n# load US states geojson data\ngeo_data = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'\n\n# create map centered on the US\nmap = folium.Map(location=[37, -102], zoom_start=4)\n\n# add heatmap layer\nfolium.Choropleth(\n    geo_data=geo_data + '/us-states.json',\n    name='choropleth',\n    data=state_salaries,\n    columns=['state', 'salary'],\n    key_on='feature.properties.name',\n    fill_color='YlOrRd',\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name='Average Salary by State (USD)',\n    tooltip=folium.features.GeoJsonTooltip(\n        fields=['name', 'salary'],\n        aliases=['State', 'Average Salary'],\n        style=\"background-color: white; color: #333333; font-family: Arial; font-size: 12px; padding: 10px;\"\n    )\n).add_to(map)\n\n# add layer control\nfolium.LayerControl().add_to(map)\n\n# show map\nmap\n\nmap.save(\"../../website/plots/plot-11.html\")\n\n\n\ndf['company_name'].value_counts()\n\nBooz Allen Hamilton                       17\nApple                                     10\nDeloitte                                   8\nWalmart                                    7\nLeidos                                     7\n                                          ..\nTrustees of University of Pennsylvania     1\nJohn Deere                                 1\nNorthwestern Mutual                        1\nMCKESSON                                   1\nBlockdaemon                                1\nName: company_name, Length: 221, dtype: int64\n\n\n\n# Get the top 5 most occurring company names\ntop_5 = df['company_name'].value_counts().head(5).index.tolist()\n\n# Get the average salary for each of the top 5 companies\nfor company in top_5:\n    avg_salary = df[df['company_name'] == company]['salary'].mean()\n    print(f\"{company}: ${avg_salary:.2f}\")\n\nBooz Allen Hamilton: $136403.12\nApple: $173942.00\nDeloitte: $149563.75\nWalmart: $nan\nLeidos: $176218.00\n\n\n\n# group by state and calculate the number of jobs\nstate_jobs = df.groupby('state')['title'].count().reset_index()\nstate_jobs.columns = ['state', 'jobs']\n\n# load US states geojson data\ngeo_data = 'https://raw.githubusercontent.com/python-visualization/folium/master/examples/data'\n\n# create map centered on the US\nmap = folium.Map(location=[37, -102], zoom_start=4)\n\n# Add heatmap layer\nfolium.Choropleth(\n    geo_data=geo_data + '/us-states.json',\n    name='choropleth',\n    data=state_jobs,\n    columns=['state', 'jobs'],\n    key_on='feature.properties.name',\n    fill_color='YlOrRd',\n    fill_opacity=0.7,\n    line_opacity=0.2,\n    legend_name='Number of Jobs by State',\n    tooltip=folium.features.GeoJsonTooltip(\n        fields=['name', 'jobs'],\n        aliases=['State', 'Number of Jobs'],\n        style=\"background-color: white; color: #333333; font-family: Arial; font-size: 12px; padding: 10px;\"\n    )\n).add_to(map)\n\n# add layer control\nfolium.LayerControl().add_to(map)\n\n# show map\nmap\n\n#map.save(\"../../website/plots/plot-16.html\")\n\n\n\n# define the trace for each degree\ntraces = []\nfor degree in df['degree'].unique():\n    trace = go.Scatter(\n        x=df[df['degree'] == degree]['experience'],\n        y=df[df['degree'] == degree]['salary'],\n        mode='markers',\n        name=degree,\n        marker=dict(size=8),\n        hovertemplate='<b>Experience Required: </b>%{x} years<br><b>Salary: </b>$%{y:,.0f}'\n    )\n    traces.append(trace)\n\nlayout = go.Layout(\n    xaxis=dict(title='Experience Required (Years)', range = [0,10]),\n    yaxis=dict(title='Annual Salary (USD)', tickformat='$,.0f'),\n    template = 'plotly_dark',\n    annotations=[\n        dict(\n            xref='paper',\n            yref='paper',\n            x=1.1,\n            y=1.2,\n            text='Explore experience and salary correlations for different degrees!',\n            showarrow=False,\n            font=dict(family='Arial', size=14, color='white'),\n            bgcolor='crimson',\n            bordercolor='crimson',\n            borderwidth=2,\n            borderpad=4,\n            xanchor='right',\n            yanchor='top'\n        )\n    ]\n)\n# create the figure object and plot it\nfig = go.Figure(data=traces, layout=layout)\nfig.show()\n\n#How do i add an annotation to the top right which says \"Play with the graph!\"\n\nfig.write_html(\"../../website/plots/plot-8.html\")\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n# group the data by 'remote'\ngrouped_df = df.groupby('remote')\n\n# create the figure object\nfig = go.Figure()\n\n# add the box traces for the remote groups\nfor remote, group_df in grouped_df:\n    fig.add_trace(go.Box(\n        y=group_df.loc[group_df['remote'] == remote, 'salary'],\n        name=str(remote),\n        hovertemplate = ('Max: $%{y:.2f}<br>' +\n                 'Upper Fence: $%{upperfence:.2f}<br>' +\n                 '75th Percentile: $%{q3:.2f}<br>' +\n                 'Median: $%{median:.2f}<br>' +\n                 '25th Percentile: $%{q1:.2f}<br>' +\n                 'Lower Fence: $%{lowerfence:.2f}<br>' +\n                 'Min: $%{min}<br>' +\n                 '<extra></extra>'),\n        marker_color='crimson' if remote else 'lightseagreen'))\n\n# create the dropdown menu options\ndropdown_options = [\n    {'label': 'Remote', 'value': 'remote'},\n    {'label': \"Bachelor's\", 'value': \"Bachelor's\"},\n    {'label': \"Master's\", 'value': \"Master's\"},\n    {'label': 'PhD', 'value': 'PhD'}\n]\n\n# define the update menu\nupdatemenu = go.layout.Updatemenu(\n    buttons=[\n        dict(label=option['label'],\n             method='update',\n             args=[{'y': [df.loc[(df['remote'] == True) & (df['degree'] == option['value']), 'salary'],\n                           df.loc[(df['remote'] == False) & (df['degree'] == option['value']), 'salary']]},\n                   {'yaxis': {'title': 'Annual Salaries for {} Holders (USD)'.format(option['value'], tickprefix = '$', tickformat=',.0f')}},\n                   {'hovertemplate': 'Max: $%{y:.0f}<br>Upper Fence: $%{upperfence:.0f}<br>Third Quartile: $%{q3:.0f}<br>Median: $%{median:.0f}<br>First Quartile: $%{q1:.0f}<br>Lower Fence: $%{lowerfence:.0f}<br>Min: $%{min:.0f}<br>Count: %{ydata:.0f}'}])\n        for option in dropdown_options[1:]\n    ],\n    direction='down',\n    showactive=True,\n    x=1.2,\n    y=1.2\n)\n\n# add the updatemenu to the layout\n\n# define the layout\nfig.update_layout(\n    xaxis=dict(title='Work Remotely?'),\n    yaxis=dict(title='Annual Salaries for Bachelor\\'s Holders (USD)', tickprefix = '$', tickformat=',.0f'),\n    template=\"plotly_dark\"\n)\n\nfig.update_layout(yaxis_tickprefix='$', yaxis_tickformat=',.0f')\n\nfig.update_layout(updatemenus=[updatemenu],    yaxis=dict(title='Annual Salaries for Bachelor\\'s Holders (USD)', tickprefix = '$', tickformat=',.0f'))\n\n# show the figure\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-9.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\ndf = pd.read_csv('../../data/job_data.csv')\n\n\n\n# drop rows with missing values in the qualifications column\ndf.dropna(subset=['qualifications'], inplace=True)\n\n# tokenize, remove stop words and lemmatize the words\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndf['qualifications'] = df['qualifications'].astype(str)\ndf['qualifications'] = df['qualifications'].apply(lambda x: [lemmatizer.lemmatize(w.lower()) for w in word_tokenize(x) if w.lower() not in stop_words])\n\n# create the word cloud\ntext = ' '.join(df['qualifications'].sum())\nwordcloud = WordCloud(width=600, height=600).generate(text)\n\n# display the word cloud\nplt.figure(figsize = (6, 6), facecolor = None)\nplt.imshow(wordcloud) #, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\nwordcloud.to_file('../../website/plots/plot-12.png')\n\n\n\n\n<wordcloud.wordcloud.WordCloud at 0x1a2393d90>\n\n\n\ndf = pd.read_csv('../../data/job_data.csv')\n\n\n\n# drop rows with missing values in the qualifications column\ndf.dropna(subset=['responsibilities'], inplace=True)\n\n# tokenize, remove stop words and lemmatize the words\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndf['responsibilities'] = df['responsibilities'].astype(str)\ndf['responsibilities'] = df['responsibilities'].apply(lambda x: [lemmatizer.lemmatize(w.lower()) for w in word_tokenize(x) if w.lower() not in stop_words])\n\n# create the word cloud\ntext = ' '.join(df['responsibilities'].sum())\nwordcloud = WordCloud(width=600, height=600).generate(text)\n\n# display the word cloud\nplt.figure(figsize = (6, 6), facecolor = None)\nplt.imshow(wordcloud) #, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\nwordcloud.to_file('../../website/plots/plot-13.png')\n\n\n\n\n<wordcloud.wordcloud.WordCloud at 0x1a2393940>\n\n\n\ndf['title'].value_counts()\n\n[data, analyst]                                                               22\n[data, scientist]                                                             21\n[machine, learning, engineer]                                                 20\n[deep, learning, engineer]                                                    14\n[blockchain, engineer]                                                        10\n                                                                              ..\n[senior, machine, learning, perception, engineer]                              1\n[hewlett, packard, lab, -, machine, learning, research, scientist, ...]        1\n[hewlett, packard, lab, -, machine, learning, research, associate, intern]     1\n[need, expert, consult, gnns, (, graph, neural, network, ...]                  1\n[natural, language, processing, engineer]                                      1\nName: title, Length: 454, dtype: int64\n\n\n\ndf = pd.read_csv('../../data/job_data.csv')\n\n\n\n# drop rows with missing values in the qualifications column\ndf.dropna(subset=['benefits'], inplace=True)\n\n# tokenize, remove stop words and lemmatize the words\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndf['benefits'] = df['benefits'].astype(str)\ndf['benefits'] = df['benefits'].apply(lambda x: [lemmatizer.lemmatize(w.lower()) for w in word_tokenize(x) if w.lower() not in stop_words])\n\n# create the word cloud\ntext = ' '.join(df['benefits'].sum())\nwordcloud = WordCloud(width=600, height=600).generate(text)\n\n# display the word cloud\nplt.figure(figsize = (6, 6), facecolor = None)\nplt.imshow(wordcloud) #, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\nwordcloud.to_file('../../website/plots/plot-14.png')\n\n\n\n\n<wordcloud.wordcloud.WordCloud at 0x1a2137d90>\n\n\n\ndf = pd.read_csv('../../data/job_data.csv')\n\n\nstate_locations = {\n    'state': ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'District of Columbia', 'Florida',\n              'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine',\n              'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska',\n              'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota',\n              'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n              'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'],\n    'latitude': [32.806671, 61.370716, 33.729759, 34.969704, 36.116203, 39.059811, 41.597782, 39.318523, 38.897438, 27.766279,\n                 33.040619, 21.094318, 44.240459, 40.349457, 39.849426, 42.011539, 38.526600, 37.668140, 31.169546,\n                 44.693947, 39.063946, 42.230171, 43.326618, 45.694454, 32.741646, 38.456085, 46.921925, 41.125370,\n                 38.313515, 43.452492, 40.298904, 34.840515, 42.165726, 35.630066, 47.528912, 40.388783, 35.565342,\n                 44.572021, 40.590752, 41.680893, 33.856892, 44.299782  , 35.747845, 31.054487  , 40.150032 , 44.045876, 37.769337, \n                 47.400902, 38.491226, 44.268543, 42.755966],\n    'longitude': [-86.791130, -152.404419, -111.431221, -92.373123, -119.681564, -105.311104, -72.755371, -75.507141, -77.026817, \n                  -81.686783, -83.643074, -157.498337, -114.478828, -88.986137, -86.258278, -93.210526, -96.726486,\n                  -84.670067, -91.867805, -69.381927, -76.802101, -71.537994, -84.536095, -93.900192, -89.398528,\n                  -92.288981, -110.454353, -99.901813, -117.055374, -71.572395, -74.405661, -106.018066, -74.948051,\n                  -79.806419, -99.784012, -82.764915, -96.928917, -122.070938, -77.209755, -77.209755, -80.945007, -99.438828, -86.692345, -97.563461, -111.862434, \n                  -72.710686, -78.169968, -121.490494, -80.954453, -89.616508,\n                  -107.302490]\n}\n\nstate_locations = pd.DataFrame(state_locations)\n\n\ndf_cleaned = df.dropna(subset=['state'])\n\n\n# merge the cleaned dataframe with the 'state_locations' dictionary\nmerged_df = df_cleaned.merge(pd.DataFrame(state_locations), on='state', how='left')\n\n\n# calculate the count of jobs for each state\njob_counts = df['state'].value_counts().reset_index()\njob_counts.columns = ['state', 'job_count']\n\n# merge the job_counts dataframe with the state_locations dictionary\nmerged_df = job_counts.merge(pd.DataFrame(state_locations), on='state', how='left')\n\n# create a map\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scattermapbox(\n        lat=merged_df['latitude'],\n        lon=merged_df['longitude'],\n        mode='markers',\n        marker=go.scattermapbox.Marker(\n            size=merged_df['job_count'],\n            sizemode='area',\n            sizeref=0.1,\n            color='crimson',\n            opacity=0.7\n        ),\n        text=merged_df['state'],\n        hovertemplate='<b>%{text}</b><br>Job Count: %{marker.size}<extra></extra>'\n    )\n)\n\nfig.update_layout(\n    mapbox_style=\"carto-positron\",\n    mapbox_zoom=3,\n    mapbox_center=dict(lat=37.0902, lon=-95.7129),\n    hovermode='closest',\n    margin=dict(l=0, r=0, t=40, b=0),\n    autosize=True,\n    showlegend=False\n)\n\nfig.show()\n\nfig.write_html(\"../../website/plots/plot-16.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "import os\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\nimport re\nimport numpy as np"
  },
  {
    "objectID": "data-cleaning.html#import-and-merge-json-files",
    "href": "data-cleaning.html#import-and-merge-json-files",
    "title": "Data Cleaning",
    "section": "1 Import and Merge JSON files",
    "text": "1 Import and Merge JSON files\nThe first step is to import all the JSON files from the 2 folders relating to DC jobs and all other jobs. We will have to import all the JSON files in each folder, merge them together, and merge the output of both folders, before converting it to a pandas DataFrame\n\ndef merge_jobs_results(directory): # merge all jsons from a folder\n    # initialize an empty list to hold the dfs\n    dfs = []\n\n    # iterate over all files in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith('.json'):\n            # read in the json data from the file\n            filepath = os.path.join(directory, filename)\n            with open(filepath, 'r') as f:\n                json_data = json.load(f)\n\n            # extract 'jobs_results' and convert to a df\n            jobs_data = json_data.get('jobs_results', [])\n            df = pd.json_normalize(jobs_data)\n            df['key'] = filename[:-7] # add column for search key, taken from file name\n\n            # add the df to the list\n            dfs.append(df)\n\n    # concatenate into a single df\n    merged_df = pd.concat(dfs, ignore_index=True)\n\n    return merged_df    \n\n\nmerged = merge_jobs_results('../../data/2023-04-14-job-search/2023-04-14-job-search-location-DC/') # DC jobs\n\n\nmerged = merged.drop('detected_extensions.commute_time', axis=1).copy() # drop last column so that both folders' files' dimensions match\n\nmerged.head(3)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      job_highlights\n      related_links\n      extensions\n      job_id\n      detected_extensions.schedule_type\n      detected_extensions.work_from_home\n      detected_extensions.posted_at\n      detected_extensions.salary\n      key\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      via Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      [{'title': 'Qualifications', 'items': ['2-3 ye...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Work from home, Full-time, No degree mentioned]\n      eyJqb2JfdGl0bGUiOiJFdGhlcmV1bSBCbG9ja2NoYWluIE...\n      Full-time\n      True\n      NaN\n      NaN\n      block-chain\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      via Greenhouse\n      We are seeking a highly motivated and skilled ...\n      [{'title': 'Qualifications', 'items': ['Bachel...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Full-time]\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIEVuZ2luZWVyIi...\n      Full-time\n      NaN\n      NaN\n      NaN\n      block-chain\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      via LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      [{'title': 'Qualifications', 'items': ['3+ yea...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [24 hours ago, Work from home, Contractor, No ...\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIENvdXJzZSBJbn...\n      Contractor\n      True\n      24 hours ago\n      NaN\n      block-chain\n    \n  \n\n\n\n\n\nmerged1 = merge_jobs_results('../../data/2023-04-14-job-search/2023-04-14-job-search-location-USA/') # USA jobs\n\n\nmerged_df = pd.concat([merged, merged1], ignore_index=True) # concatenate DC and USA jobs\nmerged_df.head(3)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      job_highlights\n      related_links\n      extensions\n      job_id\n      detected_extensions.schedule_type\n      detected_extensions.work_from_home\n      detected_extensions.posted_at\n      detected_extensions.salary\n      key\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      via Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      [{'title': 'Qualifications', 'items': ['2-3 ye...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Work from home, Full-time, No degree mentioned]\n      eyJqb2JfdGl0bGUiOiJFdGhlcmV1bSBCbG9ja2NoYWluIE...\n      Full-time\n      True\n      NaN\n      NaN\n      block-chain\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      via Greenhouse\n      We are seeking a highly motivated and skilled ...\n      [{'title': 'Qualifications', 'items': ['Bachel...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [Full-time]\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIEVuZ2luZWVyIi...\n      Full-time\n      NaN\n      NaN\n      NaN\n      block-chain\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      via LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      [{'title': 'Qualifications', 'items': ['3+ yea...\n      [{'link': 'https://www.google.com/search?hl=en...\n      [24 hours ago, Work from home, Contractor, No ...\n      eyJqb2JfdGl0bGUiOiJCbG9ja2NoYWluIENvdXJzZSBJbn...\n      Contractor\n      True\n      24 hours ago\n      NaN\n      block-chain\n    \n  \n\n\n\n\n\ndf_highlights = pd.json_normalize(merged_df['job_highlights']) # break up job highlights\ndf_highlights.head(5)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      {'title': 'Qualifications', 'items': ['2-3 yea...\n      {'title': 'Responsibilities', 'items': ['Desig...\n      None\n    \n    \n      1\n      {'title': 'Qualifications', 'items': ['Bachelo...\n      {'title': 'Responsibilities', 'items': ['As a ...\n      {'title': 'Benefits', 'items': ['(NYC only) Pu...\n    \n    \n      2\n      {'title': 'Qualifications', 'items': ['3+ year...\n      {'title': 'Responsibilities', 'items': ['Our e...\n      None\n    \n    \n      3\n      {'title': 'Qualifications', 'items': ['Candida...\n      {'title': 'Responsibilities', 'items': ['Will ...\n      None\n    \n    \n      4\n      {'title': 'Qualifications', 'items': ['You are...\n      {'title': 'Responsibilities', 'items': ['To bu...\n      None"
  },
  {
    "objectID": "data-cleaning.html#clean-data",
    "href": "data-cleaning.html#clean-data",
    "title": "Data Cleaning",
    "section": "2 Clean Data",
    "text": "2 Clean Data\nNow, we will clean all the necessary columns of our data, and extract relevant data\n\ndef get_items_value(dictionary): # get item value\n    if pd.isnull(dictionary): # deal with na values\n        return None\n    return dictionary['items']\n\ndf_highlights = df_highlights.applymap(get_items_value) # only get the description of the item\n\n\ndf_highlights = df_highlights.rename(columns={0: 'qualifications', 1: 'responsibilities', 2: 'benefits'}) # rename columns\nprint(df_highlights.isna().sum())\n\nqualifications        0\nresponsibilities    105\nbenefits            400\ndtype: int64\n\n\n\nmaster_df = pd.concat([merged_df, df_highlights], axis = 1) # merge both dataframes by column\nmaster_df = master_df.drop(['job_highlights', 'related_links', 'job_id', 'detected_extensions.work_from_home', 'detected_extensions.posted_at'], axis = 1) # drop unneeded columns\n\nfor i in master_df: # check for nas\n    print(i, master_df[i].isna().sum())\n\ntitle 0\ncompany_name 0\nlocation 0\nvia 0\ndescription 0\nextensions 0\ndetected_extensions.schedule_type 1\ndetected_extensions.salary 690\nkey 0\nqualifications 0\nresponsibilities 105\nbenefits 400\n\n\n\n#rename columns\nmaster_df = master_df.rename(columns={'detected_extensions.schedule_type': 'schedule_type', 'detected_extensions.salary': 'salary', 'key': 'query'})\n\nmaster_df = master_df.reset_index(drop=True)\n\nmaster_df.head(5)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      extensions\n      schedule_type\n      salary\n      query\n      qualifications\n      responsibilities\n      benefits\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      via Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      [Work from home, Full-time, No degree mentioned]\n      Full-time\n      NaN\n      block-chain\n      [2-3 years of Software Development experience,...\n      [Design, maintain and deploy smart contracts f...\n      None\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      via Greenhouse\n      We are seeking a highly motivated and skilled ...\n      [Full-time]\n      Full-time\n      NaN\n      block-chain\n      [Bachelor's or Master's degree in Computer Sci...\n      [As a Blockchain Engineer, you will be respons...\n      [(NYC only) Pursuant to Section 8-102 of title...\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      via LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      [24 hours ago, Work from home, Contractor, No ...\n      Contractor\n      NaN\n      block-chain\n      [3+ years of experience in blockchain, cryptoc...\n      [Our expert technical team will provide the su...\n      None\n    \n    \n      3\n      Python based - Blockchain developer to join ex...\n      Upwork\n      Anywhere\n      via Upwork\n      Need someone to join our existing team to spee...\n      [2 days ago, 10–30 an hour, Work from home, Co...\n      Contractor\n      10–30 an hour\n      block-chain\n      [Candidates must be willing to sign, non-discl...\n      [Will discuss details with the selected candid...\n      None\n    \n    \n      4\n      Blockchain DevOps Engineer (Remote)\n      Telnyx\n      United States\n      via Startup Jobs\n      About Telnyx\\n\\nAt Telnyx, we’re architecting ...\n      [4 days ago, Full-time, No degree mentioned]\n      Full-time\n      NaN\n      block-chain\n      [You are a highly motivated and experienced Bl...\n      [To build a best-in-class Filecoin (FIL) Minin...\n      None\n    \n  \n\n\n\n\n\nmaster_df['query'] = master_df['query'].str.replace('-', ' ') # replace hyphens with spaces\nmaster_df.loc[master_df['query'] == 'block chain', 'query'] = 'blockchain' # remove space from blockchain\n\n\nmaster_df['via'] = master_df['via'].str.slice(4) # remove 'via' from the start of the string\n\nmaster_df = master_df.join(master_df['extensions'].apply(lambda x: pd.Series(x)).add_prefix('ext_'))\nmaster_df = master_df.drop('extensions', axis=1)\n\n\nmaster_df.head(5)\n\n\n\n\n\n  \n    \n      \n      title\n      company_name\n      location\n      via\n      description\n      schedule_type\n      salary\n      query\n      qualifications\n      responsibilities\n      benefits\n      ext_0\n      ext_1\n      ext_2\n      ext_3\n      ext_4\n      ext_5\n      ext_6\n      ext_7\n    \n  \n  \n    \n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      Full-time\n      NaN\n      blockchain\n      [2-3 years of Software Development experience,...\n      [Design, maintain and deploy smart contracts f...\n      None\n      Work from home\n      Full-time\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      Greenhouse\n      We are seeking a highly motivated and skilled ...\n      Full-time\n      NaN\n      blockchain\n      [Bachelor's or Master's degree in Computer Sci...\n      [As a Blockchain Engineer, you will be respons...\n      [(NYC only) Pursuant to Section 8-102 of title...\n      Full-time\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      Contractor\n      NaN\n      blockchain\n      [3+ years of experience in blockchain, cryptoc...\n      [Our expert technical team will provide the su...\n      None\n      24 hours ago\n      Work from home\n      Contractor\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      Python based - Blockchain developer to join ex...\n      Upwork\n      Anywhere\n      Upwork\n      Need someone to join our existing team to spee...\n      Contractor\n      10–30 an hour\n      blockchain\n      [Candidates must be willing to sign, non-discl...\n      [Will discuss details with the selected candid...\n      None\n      2 days ago\n      10–30 an hour\n      Work from home\n      Contractor\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Blockchain DevOps Engineer (Remote)\n      Telnyx\n      United States\n      Startup Jobs\n      About Telnyx\\n\\nAt Telnyx, we’re architecting ...\n      Full-time\n      NaN\n      blockchain\n      [You are a highly motivated and experienced Bl...\n      [To build a best-in-class Filecoin (FIL) Minin...\n      None\n      4 days ago\n      Full-time\n      No degree mentioned\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nmaster_df['schedule_type'].unique()\n\narray(['Full-time', 'Contractor', 'Internship', 'Part-time', nan],\n      dtype=object)\n\n\n\n# loop through each row in the dataframe to delete 'days ago' 'hours ago'\nfor i, row in master_df.iterrows():\n    # check if 'ago' is present in ext_0\n    if 'ago' in str(row['ext_0']):\n        # if it is, replace the 'ago' value with the corresponding value from ext_1\n        master_df.at[i, 'ext_0'] = row['ext_1']\n        master_df.at[i, 'ext_1'] = np.nan\n    else:\n        # if it isn't, keep the row as it is\n        pass\n\n\nmaster_df.isna().sum()\n\ntitle                 0\ncompany_name          0\nlocation              0\nvia                   0\ndescription           0\nschedule_type         1\nsalary              690\nquery                 0\nqualifications        0\nresponsibilities    105\nbenefits            400\next_0                 0\next_1               641\next_2               343\next_3               530\next_4               670\next_5               778\next_6               812\next_7               819\ndtype: int64\n\n\n\nmaster_df = master_df.drop(['ext_0', 'ext_1', 'ext_2', 'ext_3', 'ext_4', 'ext_5', 'ext_6', 'ext_7'], axis = 1)\n\n\nfor i, row in master_df.iterrows(): # go through responsibilities and benefits column to extract salary\n    if pd.isna(row['salary']):\n        if '$' in str(row['responsibilities']):\n            master_df.at[i, 'salary'] = row['responsibilities']\n        elif 'hour' in str(row['responsibilities']):\n            master_df.at[i, 'salary'] = row['responsibilities']\n        elif 'year' in str(row['responsibilities']):\n            master_df.at[i, 'salary'] = row['responsibilities']\n        elif '$' in str(row['benefits']):\n            master_df.at[i, 'salary'] = row['benefits']\n        elif 'hour' in str(row['benefits']):\n            master_df.at[i, 'salary'] = row['benefits']\n        elif 'year' in str(row['benefits']):\n            master_df.at[i, 'salary'] = row['benefits']\n\n\nmaster_df['degree'] = np.nan\n\n\nfor i, row in master_df.iterrows(): # go through responsibilities and benefits column to extract salary\n    if pd.isna(row['degree']):\n        if \"PhD\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"PhD\"\n        elif \"Ph.D.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"PhD\"\n        elif \"Doctorate\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"PhD\"\n        elif \"Master\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Master's\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Masters\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Msc\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"M.A.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"MA\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"MS\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Advanced\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"M.S.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Master's\"\n        elif \"Bachelor's\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"BS\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"B.S.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"BA\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"B.A.\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"Bachelors\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"Bachelor\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n        elif \"Undergraduate\" in str(row['qualifications']):\n            master_df.at[i, 'degree'] = \"Bachelor's\"\n\n\nmaster_df['salary'] = master_df['salary'].apply(lambda x: str(x) if isinstance(x, list) else x) # convert salary to string \n\n\ndef clean_salary(salary):\n    # if salary is a list, extract the salary range from the appropriate string\n    if isinstance(salary, list):\n        for s in salary:\n            if \"salary range\" in s.lower():\n                salary = re.findall(r\"\\$\\d+(?:,\\d+)*(?:\\.\\d+)?\", s)\n                salary = \" - \".join(salary)\n                break\n    # if salary is a string, extract the salary range\n    elif isinstance(salary, str):\n        salary = re.findall(r\"\\$\\d+(?:,\\d+)*(?:\\.\\d+)?\", salary)\n        salary = \" - \".join(salary)\n    else:\n        salary = np.nan\n    return salary\n\ndef extract_time_period(salary):\n    if isinstance(salary, str):\n        if \"hour\" in salary:\n            time_period = \"an hour\"\n        elif \"month\" in salary:\n            time_period = \"a month\"\n        elif \"year\" in salary:\n            time_period = \"a year\"\n        else:\n            time_period = np.nan\n    else:\n        time_period = np.nan\n    return time_period\n\nmaster_df[\"salary_cleaned\"] = master_df[\"salary\"].apply(clean_salary)\nmaster_df[\"time_period\"] = master_df[\"salary_cleaned\"].apply(extract_time_period)\n\n\n# iterate over every row in the dataframe\nfor index, row in master_df.iterrows():\n    # check if the 'salary_cleaned' value contains '$'\n    if '$' in str(row['salary_cleaned']):\n        # assign the entire value to the 'salary' column\n        master_df.at[index, 'salary'] = row['salary_cleaned'] #####\n\n\n# remove list values from salary\n# create a boolean mask to filter out missing values \nnot_null_mask = pd.notnull(master_df['salary']) \n\n# find the rows where 'salary' contains '['\nrows_with_brackets = master_df[not_null_mask & master_df['salary'].str.contains('\\\\[')]\n\n# set the 'salary' column to NaN for the rows with brackets\nmaster_df.loc[rows_with_brackets.index, 'salary'] = np.nan\n\n\nmaster_df['salary'] = master_df['salary'].str.replace('–', '-')\n\n\n# create a boolean mask to filter for rows containing 'hour'\nmask = master_df['salary'].str.contains('hour', na=False)\n\n# select the rows where 'salary' contains 'hour'\nrows_with_hourly_wage = master_df[mask]\n\n# convert hourly wage to annual salary\nfor idx, row in rows_with_hourly_wage.iterrows():\n    salary_str = row['salary']\n    if salary_str.count('-') == 1:\n        # handle case where salary range is given\n        salary_range = salary_str.split(' ')[0]\n        salary_range = salary_range.replace('\\u2011', '-')\n        start, end = map(float, salary_range.split('-'))\n        avg_salary = (start + end) / 2.0\n        annual_salary = avg_salary * 2080\n        master_df.loc[idx, 'salary'] = '${:,.2f}'.format(annual_salary)\n    elif salary_str.count('-') == 0:\n        # handle case where single hourly wage is given\n        hourly_wage_str = salary_str.split(' ')[0]\n        hourly_wage = float(hourly_wage_str)\n        annual_salary = hourly_wage * 2080\n        master_df.loc[idx, 'salary'] = '${:,.2f}'.format(annual_salary)\n\n\nmaster_df['salary'] = master_df['salary'].str.replace(' a year', '').str.strip()\nmaster_df['salary'] = master_df['salary'].str.replace('$', '').str.strip()\nmaster_df.loc[636, 'salary'] = 136284\nmaster_df.loc[761, 'salary'] = 22070\nmaster_df.loc[681, 'salary'] = 121000\nmaster_df.loc[746, 'salary'] = 206000\n\n/var/folders/37/fsk42jds3255qblrs5r1v99c0000gn/T/ipykernel_40882/4119749001.py:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n  master_df['salary'] = master_df['salary'].str.replace('$', '').str.strip()\n\n\n\nnum_nas = master_df['salary'].isna().sum()\n\nprint(f\"Number of NaN values in 'salary' column: {num_nas}\")\n\nNumber of NaN values in 'salary' column: 473\n\n\n\n# define a function to convert salary ranges to their average values\nmaster_df['salary'] = master_df['salary'].astype(str)\n\ndef parse_salary_range(s):\n    if '-' in s:\n        s = s.replace('$', '').replace(',', '').replace('K', '000').replace('k', '000')\n        parts = s.split('-')\n        parts = [float(p) for p in parts if not pd.isnull(p)]\n        avg = sum(parts) / len(parts)\n        return avg\n    else:\n        return s\n\n# apply the function to the 'salary' column of master_df\nmaster_df['salary'] = master_df['salary'].apply(parse_salary_range)\n\n\nmaster_df['salary'] = master_df['salary'].astype(str)  # convert all data points to string type\nmaster_df['salary'] = master_df['salary'].str.replace(',', '')  # remove commas\nmaster_df['salary'] = master_df['salary'].str.replace('\\.\\d+', '')  # remove decimals and everything after them\nmaster_df['salary'] = pd.to_numeric(master_df['salary'], errors='coerce')\nmaster_df['salary'].describe()\n\n/var/folders/37/fsk42jds3255qblrs5r1v99c0000gn/T/ipykernel_40882/1948180007.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n  master_df['salary'] = master_df['salary'].str.replace('\\.\\d+', '')  # remove decimals and everything after them\n\n\ncount       347.000000\nmean     125612.092219\nstd       73116.788777\nmin           0.000000\n25%       59013.000000\n50%      134500.000000\n75%      175000.000000\nmax      400000.000000\nName: salary, dtype: float64\n\n\n\nmaster_df['qualifications'] = master_df['qualifications'].astype(str)\n\ndef extract_experience(text):\n    match = re.search(r'(\\d[\\d+-]*\\s*(?:year|yr|yrs|years))', text, re.IGNORECASE)\n    if match:\n        return match.group(1)\n    else:\n        return None\n\nmaster_df['experience'] = master_df['qualifications'].apply(extract_experience)\n\n\nmaster_df['experience'] = master_df['experience'].apply(lambda x: str(x))\n\ndef clean_experience(exp):\n    if isinstance(exp, str):\n        # extract digits from the experience string\n        digits = re.findall('\\d+', exp)\n        if len(digits) == 2:\n            # calculate the average of two numbers if there are two digits\n            avg_exp = (int(digits[0]) + int(digits[1])) / 2\n        elif len(digits) == 1:\n            # take the single digit if there is only one digit\n            avg_exp = int(digits[0])\n        else:\n            # return None if there are no digits\n            return None\n        return avg_exp\n    else:\n        # return None if the input is not a string\n        return None\n\nmaster_df['experience'] = master_df['experience'].apply(clean_experience)\n\n\nmaster_df = master_df.drop(['salary_cleaned', 'time_period'], axis = 1)\n\n\nmaster_df.isna().sum()\n\ntitle                 0\ncompany_name          0\nlocation              0\nvia                   0\ndescription           0\nschedule_type         1\nsalary              476\nquery                 0\nqualifications        0\nresponsibilities    105\nbenefits            400\ndegree              287\nexperience          436\ndtype: int64\n\n\n\nmin_count = master_df.count().min()\nprint(\"Number of rows with entries for every column:\", min_count)\n\nNumber of rows with entries for every column: 347\n\n\n\nmaster_df['degree'] = master_df['degree'].astype('category')\n\n\nmaster_df.dtypes\n\ntitle                 object\ncompany_name          object\nlocation              object\nvia                   object\ndescription           object\nschedule_type         object\nsalary               float64\nquery                 object\nqualifications        object\nresponsibilities      object\nbenefits              object\ndegree              category\nexperience           float64\ndtype: object\n\n\n\nmaster_df['location'] = master_df['location'].str.strip() # delete extra spaces\nmaster_df['remote'] = master_df['location'].apply(lambda x: True if x in ['Anywhere', 'United States'] else False) # make 'remote' dummy variable\n\n\nmaster_df.loc[[26, 58, 73, 77, 127, 230, 271, 322, 463, 472, 651, 731, 763, 768, 781], 'salary'] = np.nan # delete bonuses that were detected as salaries\n\nmaster_df.loc[[48, 110, 193, 264, 614, 654], 'salary'] *= 2080 # convert unconverted hourly salaries\n\nmaster_df.loc[[108, 143, 151, 231, 249, 340, 350, 357, 396, 426, 525, 542, 557, 627, 737, 758, 808], 'salary'] *= 1000 # salaries read as 240 instead of 240000\n\nmaster_df.loc[[251, 696], 'salary'] *= 12 # convert unconverted monthly salaries\n\n\nmaster_df['query'] = master_df['query'].apply(lambda x: ' '.join(word if word == 'and' else word.title() for word in x.split()))\n\n\ndef clean_location(location):\n    # find the index of the '(' character\n    index = location.find('(')\n    if index != -1:\n        # if '(' is found, remove it and everything that comes after it\n        location = location[:index].strip()\n    # remove extra spaces at the start and end of the string\n    return location.strip()\n\nmaster_df['location'] = master_df['location'].apply(clean_location)\n\n\n# define a function to extract city and state information from the location string\ndef get_city_state(location):\n    # split the location string by ','\n    parts = location.split(',')\n    if len(parts) == 2:\n        # if the location has two parts, assume the first is the city and the second is the state\n        city = parts[0].strip()\n        state = parts[1].strip()\n        return city, state\n    elif len(parts) == 1:\n        # if the location has only one part, assume it is the state\n        state = parts[0].strip()\n        if location == 'Anywhere' or location == 'United States':\n            return '', ''\n        else:\n            return '', state\n    else:\n        # if the location has more than two parts, assume it is not a valid city, state format\n        return '', ''\n\n# apply the function to the location column\nmaster_df[['city', 'state']] = master_df['location'].apply(lambda x: pd.Series(get_city_state(x)))\n\n# handle special case for state abbreviations\nmaster_df['state'] = master_df['state'].apply(lambda x: 'TX' if x == 'Texas' else x)\n\n\n# define a dictionary mapping state abbreviations to their full form\nstate_abbreviations = {\n    'AL': 'Alabama', \n    'AK': 'Alaska', \n    'AZ': 'Arizona', \n    'AR': 'Arkansas', \n    'CA': 'California', \n    'CO': 'Colorado', \n    'CT': 'Connecticut', \n    'DE': 'Delaware', \n    'DC': 'District of Columbia', \n    'FL': 'Florida', \n    'GA': 'Georgia', \n    'HI': 'Hawaii', \n    'ID': 'Idaho', \n    'IL': 'Illinois', \n    'IN': 'Indiana', \n    'IA': 'Iowa', \n    'KS': 'Kansas', \n    'KY': 'Kentucky', \n    'LA': 'Louisiana', \n    'ME': 'Maine', \n    'MD': 'Maryland', \n    'MA': 'Massachusetts', \n    'MI': 'Michigan', \n    'MN': 'Minnesota', \n    'MS': 'Mississippi', \n    'MO': 'Missouri', \n    'MT': 'Montana', \n    'NE': 'Nebraska', \n    'NV': 'Nevada', \n    'NH': 'New Hampshire', \n    'NJ': 'New Jersey', \n    'NM': 'New Mexico', \n    'NY': 'New York', \n    'NC': 'North Carolina', \n    'ND': 'North Dakota', \n    'OH': 'Ohio', \n    'OK': 'Oklahoma', \n    'OR': 'Oregon', \n    'PA': 'Pennsylvania', \n    'RI': 'Rhode Island', \n    'SC': 'South Carolina', \n    'SD': 'South Dakota', \n    'TN': 'Tennessee', \n    'TX': 'Texas', \n    'UT': 'Utah', \n    'VT': 'Vermont', \n    'VA': 'Virginia', \n    'WA': 'Washington', \n    'WV': 'West Virginia', \n    'WI': 'Wisconsin', \n    'WY': 'Wyoming'\n}\n\n# apply the mapping to the 'state' column\nmaster_df['state'] = master_df['state'].apply(lambda x: state_abbreviations.get(x, x))"
  },
  {
    "objectID": "data-cleaning.html#output-cleaned-csv",
    "href": "data-cleaning.html#output-cleaned-csv",
    "title": "Data Cleaning",
    "section": "3 Output Cleaned CSV",
    "text": "3 Output Cleaned CSV\n\nmaster_df.to_csv('job_data.csv')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Data Job Market in the US",
    "section": "",
    "text": "The job market in the US has been quite unstable recently, with companies, especially technology ones, firing employees left, right and center in order to reduce costs and human power. To add on to that, widespread inflation in the US has led to jobs overpaying several employees, which is another contributing factor to mass layoffs in the technology sector. This project is an attempt to portray a deep dive into the market for data-related jobs in the US, by means of effective data visualizations. We will explore several features of jobs, including their salary, experience, qualifications, and locations, and we will do this in a visually informative manner. I hope you enjoy my project.\n\n\n\nIn this section, we will first look at some of the higher-level information of the jobs available to us, before diving deeper into the nitty-gritty details. We will look at the cities and states with the most jobs, and also explore common job titles and search engines.\n\n\n\n\n\n\n        \n        \nFigure 1: shows a geospatial map depicting the number of job searches per US state, for which job data is available.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 2: shows a bar plot which plots the 5 most frequent locations, and the number of times they appear in the search.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 3: shows a bar plot which plots the 5 most frequent job posters, and the number of times they appear in the search.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 4: shows a bar plot which plots the search queries, and the number of times they appear based on the degree required.\n\n\n\nFrom the above plots, we see that:\n\n\nThe distribution of jobs among different states is not uniform. We notice that states such as New York, California and the District of Columbia have the most jobs, which is a pattern that we would expect to see, given that half of our job searches come from the DMV, and given that California and New York are both hubs for technology and business alike.\n\n\nIn terms of cities, New York and Washington reign supreme, and surprisingly enough, San Francisco comes in with the third most jobs of our dataset.\n\n\nThe majority of our jobs come from LinkedIn, which is a very popular job search engine.\n\n\n\n\n\n\nThe data domain is indubitably a lucrative domain to be in, which is why jobs are hard to find and secure. While the pay and lifestyle is lavish, breaking into the industry and securing a job takes a lot more than simply a degree related to a data-related subject. Therefore, in this section, we will explore jobs requirements, such as minimum degrees and years of experience, and also explore the perks that come with jobs, including high salaries and opportunities to work from home.\nLet’s explore the average salaries and experience required for data-related jobs, for each state. Figure 5:\n\n\n\n\n\n\n\n        \n        \nFigure 5: shows an interactive table with an option to select between average yearly salary and experience required by state.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFrom the above interactive data table, we can gain a better understanding into whether different states look for different levels of experience or pay differing amounts in salaries to employees. Note that some values in the table may be skewed due to limited job postings for some states.\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 6: shows the relationship between experience required and salary, by degree required.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 7: shows a histogram of the salaries of job results from the search.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 8: shows a histogram of the years of experience required in job results from the search.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 9: shows a bar plot depicting the average salaries for the most popular locations in the search.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 10: shows a pie chart with the distribution of the jobs results for whether they were in-person or remote.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 11: shows the salaries for both in person and work from home jobs, by degree required.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 12: shows a grouped bar plot the salaries for varying years of experience, by degree required.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 13: shows a word cloud for the most common words found in ‘benefits’.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 14: shows the average salaries for the 5 most popular job titles.\n\n\n\nSo, what can we make from examining all the above plots? We see that:\n\n\nIn the datatable, the highest salaries are in states like New York and California, which is an expected result. This is likely due to the high tax rates and living costs in those states. We also notice a lot of states with no results, which can raise questions as to how representative the industry really is in terms of opportunities to work across the country.\n\n\nOur word cloud yields several benefits that job seekers can potentially enjoy: parental leave, paid time off, and several different types of insurance including dental and vision.\n\n\nThe correlation plot between experience required and salary exhibits a medium strength positive correlation between experience and salary. However, upon manipulation of the plot, we notice that PhDs have a steeper rise in salary for an increase in experience, on average, than Bachelor’s holders. Furthermore, we see that while jobs boast high salaries, averaging $150,000 to $200,000, they also expect, on average, 4-6 years of experience.\n\n\nExploring the remote work environment, the majority of jobs in our dataset do not include work from home options. We also see how in-person jobs, on average, have lower salaries than remote jobs, which is a surprising statistic.\n\n\n\n\n\n\nFor this section, we’ll look into other features of jobs, including the most popular titles, and the qualifications and responsibilities that are looked for. Since job sections like qualifications and responsibilities are normally sections that are filled with dense text, they are often overlooked by job-seekers. In fact, they contain the crucial elements of what the job-seeker should look to exhibit to their potential employer to maximize their chances of getting hired.\n\n\n\n\n\n\n        \n        \nFigure 15: shows a bar plot which plots the 5 most frequent job titles, and the number of times they appear in the search.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 16: shows a word cloud for the most common words found in ‘qualifications’.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 17: shows a word cloud for the most common words found in ‘responsibilities’.\n\n\n\n\n\n\n\n\n\n\n        \n        \nFigure 18: shows a NetworkX network connecting job queries and titles, depicting the extent of data-related jobs.\n\n\n\nKey takeaways from the above section:\n\n\nData Scientist and Data Analyst are the most common job titles from our entire search. This might indicate that more generalist positions are more readily available in the job market in comparison to other, more specialised positions.\n\n\nMost of these are skills in the word cloud on qualifications are those that we’d expect to see: machine learning, big data, computer science, and deep learning.\n\n\nThe word cloud on responsibilities gives an overview of what a day to day on the job might look like: project, team, research, analysis, and development are words that can throw light on the daily job responsibilities.\n\n\nOur network allows us to visualize the most common job titles and queries, elucidating the domains that are in high demand.\n\n\n\n\n\nThrough this visual project, we explored several facets of the current state of the market for data jobs in the US. We explored the data spatially, looking at which states had the most number of jobs available, and which states had the highest average salary. We also explored the most common cities and their average salaries. Some of the more insightful takeaways from a holistic view of the visualizations include:\n\n\nOf the most common job titles, Blockchain Engineer and Deep Learning Engineer reigned supreme with the highest salaries, averaging $166,000per year.\n\n\nAlthough they were the 2 most common job titles from the entire dataset, the job titles Data Scientist and Data Analyst had considerably lower salaries, averaging $110,000 per year, which might indicate that taking up a job which requires specialization would lead to a higher pay.\n\n\nHowever, most of these specialized jobs require at least a Master’s and most often a PhD, degree, which tend to be an extreme expense in terms of time, tuition money, and foregone salary.\n\n\nOn the other hand, the benefits of having a PhD are also noticed with the steep increase in job salary for an increase in experience, in comparison to other degree holders.\n\n\nExploring remote and in-person options, we saw that while in-person jobs were a lot more available, remote jobs paid more, on average.\n\n\nFinally, it is worth mentioning that the conclusions of this project are only limited to the data used for analysis, so it is difficult to establish causality or relationships for all job postings solely based off this particular dataset.\nThank you for taking the time to view my project. I hope you enjoyed going through it as much as I enjoyed creating it!"
  },
  {
    "objectID": "vis_r.html",
    "href": "vis_r.html",
    "title": "R",
    "section": "",
    "text": "library(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n──\n\n\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n✔ purrr   1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks plotly::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(htmlwidgets)\nlibrary(dplyr)\noptions(warn = - 1)\nlibrary(withr)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# lubridate, zoo, xts\noptions(scipen=999)\nlibrary(reshape2)\n\n\nAttaching package: 'reshape2'\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\n\njobs_df <- read_csv('../../data/job_data.csv')\n\nNew names:\nRows: 823 Columns: 17\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(13): title, company_name, location, via, description, schedule_type, qu... dbl\n(3): ...1, salary, experience lgl (1): remote\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\n\nhead(jobs_df)\n\n# A tibble: 6 × 17\n   ...1 title compa…¹ locat…² via   descr…³ sched…⁴ salary query quali…⁵ respo…⁶\n  <dbl> <chr> <chr>   <chr>   <chr> <chr>   <chr>    <dbl> <chr> <chr>   <chr>  \n1     0 Ethe… Ex Pop… Anywhe… Buil… \"Compa… Full-t…     NA Bloc… \"['2-3… ['Desi…\n2     1 Bloc… 21.co   New Yo… Gree… \"We ar… Full-t… 180000 Bloc… \"[\\\"Ba… ['As a…\n3     2 Bloc… Blockc… Anywhe… Link… \"Are y… Contra…     NA Bloc… \"['3+ … ['Our …\n4     3 Pyth… Upwork  Anywhe… Upwo… \"Need … Contra…  41600 Bloc… \"['Can… ['Will…\n5     4 Bloc… Telnyx  United… Star… \"About… Full-t…     NA Bloc… \"['You… ['To b…\n6     5 Ethe… Turnbl… Anywhe… ZipR… \"Our c… Full-t… 150000 Bloc… \"['5+ … ['As a…\n# … with 6 more variables: benefits <chr>, degree <chr>, experience <dbl>,\n#   remote <lgl>, city <chr>, state <chr>, and abbreviated variable names\n#   ¹​company_name, ²​location, ³​description, ⁴​schedule_type, ⁵​qualifications,\n#   ⁶​responsibilities\n\n\n\n# create a new column 'experience_group' based on 'experience'\njobs_df$experience_group <- cut(jobs_df$experience, c(0, 5, 10, Inf),\n                                 labels = c('0-5', '5-10', '10+'))\n\n\njobs_df_pivot <- jobs_df %>%\n  group_by(degree, experience_group) %>%\n  summarise(mean_salary = mean(salary, na.rm = TRUE)) %>%\n  pivot_wider(names_from = experience_group, values_from = mean_salary)\n\n`summarise()` has grouped output by 'degree'. You can override using the\n`.groups` argument.\n\n\n\njobs_df_pivot <- jobs_df_pivot[1:(nrow(jobs_df_pivot)-1), 1:(ncol(jobs_df_pivot)-1)]\n\n\n# Create the plotly plot\nap <- jobs_df_pivot %>% plot_ly(\n    hovertext = \"Degree, Salary\"\n)  \n\nap <- ap %>%\n  add_trace(x = jobs_df_pivot$degree, y = ~`0-5`, type = 'bar', name = '0-5', marker = list(color = '#118C4F'))\n\nap <- ap %>%\n  add_trace(x = jobs_df_pivot$degree, y = ~`5-10`, type = 'bar', name = '5-10', marker = list(color = '#FFB90D'))\n\nap <- ap %>%\n  add_trace(x = jobs_df_pivot$degree, y = ~`10+`, type = 'bar', name = '10+' , marker = list(color = 'red'))\n\n# Set the plot layout and theme\nap <- ap %>% \n  layout(\n    title = list(text = \"\", y = 0.98, font = list(family = \"Arial\", size = 18, color = \"white\")),\n    xaxis = list(title = list(text = \"Degree\", font = list(family = \"Arial\", color = \"white\")), \n                 tickfont = list(color = \"white\")),\n    yaxis = list(title = list(text = \"Salary (USD)\", font = list(family = \"Arial\", color = \"white\")),\n                 tickprefix = \"$\", tickformat = \",\",\n                 tickfont = list(color = \"white\")),\n    legend = list(title = list(text = \"Years of Experience\", font = list(color = \"white\")), \n                  font = list(color = \"white\")),\n    template = \"plotly_dark\",\n    paper_bgcolor = \"black\",\n    plot_bgcolor = \"black\"\n  )\n\n\n# Show the plot\nap\n\n\n\n\nsaveWidget(ap, file = \"../../website/plots/plot-10.html\")\n\n\n# Calculate the count of remote and non-remote jobs\nremote_count <- c(sum(jobs_df$remote == TRUE), sum(jobs_df$remote == FALSE))\n\n# Create a pie chart with Plotly\npc <- plot_ly(labels = c(\"Remote\", \"Non-Remote\"), values = remote_count, type = \"pie\",\n        textinfo = \"value+percent\", hole = 0.6,\n        marker = list(colors = c(\"#00CC96\", \"#EF553B\"))) %>%\n  layout(font = list(family = \"Arial\", color = \"white\"),\n         textfont = list(color = \"white\"),\n         paper_bgcolor = \"black\",\n        plot_bgcolor = \"black\")\n\npc \n\n\n\n\nsaveWidget(pc, file = \"../../website/plots/plot-15.html\")"
  },
  {
    "objectID": "data-exploration.html",
    "href": "data-exploration.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv('../../data/job_data.csv')"
  },
  {
    "objectID": "data-exploration.html#summary-statistics",
    "href": "data-exploration.html#summary-statistics",
    "title": "Exploratory Data Analysis",
    "section": "2 Summary Statistics",
    "text": "2 Summary Statistics\nThe first step of EDA is to alway look at the structure of our dataframe, and key statistics of both numerical and categorical variables. Our dataset on jobs is one which has several types of data, which is why there is a breadth of exploratory data analysis that can be conducted on this particular dataset. Summary statistics are an important aspect of exploratory data analysis because they provide a concise and comprehensive summary of the key features and characteristics of our data set. By calculating summary statistics, we can quickly gain insights into the central tendency, variability, and distribution of the data.\n\ndf.shape\n\n(823, 17)\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      title\n      company_name\n      location\n      via\n      description\n      schedule_type\n      salary\n      query\n      qualifications\n      responsibilities\n      benefits\n      degree\n      experience\n      remote\n      city\n      state\n    \n  \n  \n    \n      0\n      0\n      Ethereum Blockchain Developer (Remote)\n      Ex Populus\n      Anywhere\n      Built In\n      Company Overview:\\nEx Populus is a cutting-edg...\n      Full-time\n      NaN\n      Blockchain\n      ['2-3 years of Software Development experience...\n      ['Design, maintain and deploy smart contracts ...\n      NaN\n      NaN\n      2.5\n      True\n      NaN\n      NaN\n    \n    \n      1\n      1\n      Blockchain Engineer\n      21.co\n      New York, NY\n      Greenhouse\n      We are seeking a highly motivated and skilled ...\n      Full-time\n      180000.0\n      Blockchain\n      [\"Bachelor's or Master's degree in Computer Sc...\n      ['As a Blockchain Engineer, you will be respon...\n      ['(NYC only) Pursuant to Section 8-102 of titl...\n      Master's\n      NaN\n      False\n      New York\n      New York\n    \n    \n      2\n      2\n      Blockchain Course Instructor\n      Blockchain Institute of Technology\n      Anywhere\n      LinkedIn\n      Are you a blockchain, cryptocurrency, NFT, Met...\n      Contractor\n      NaN\n      Blockchain\n      ['3+ years of experience in blockchain, crypto...\n      ['Our expert technical team will provide the s...\n      NaN\n      NaN\n      3.0\n      True\n      NaN\n      NaN\n    \n    \n      3\n      3\n      Python based - Blockchain developer to join ex...\n      Upwork\n      Anywhere\n      Upwork\n      Need someone to join our existing team to spee...\n      Contractor\n      41600.0\n      Blockchain\n      ['Candidates must be willing to sign, non-disc...\n      ['Will discuss details with the selected candi...\n      NaN\n      NaN\n      NaN\n      True\n      NaN\n      NaN\n    \n    \n      4\n      4\n      Blockchain DevOps Engineer (Remote)\n      Telnyx\n      United States\n      Startup Jobs\n      About Telnyx\\n\\nAt Telnyx, we’re architecting ...\n      Full-time\n      NaN\n      Blockchain\n      ['You are a highly motivated and experienced B...\n      ['To build a best-in-class Filecoin (FIL) Mini...\n      NaN\n      Bachelor's\n      NaN\n      True\n      NaN\n      NaN\n    \n  \n\n\n\n\n\ndf.dtypes\n\nUnnamed: 0            int64\ntitle                object\ncompany_name         object\nlocation             object\nvia                  object\ndescription          object\nschedule_type        object\nsalary              float64\nquery                object\nqualifications       object\nresponsibilities     object\nbenefits             object\ndegree               object\nexperience          float64\nremote                 bool\ncity                 object\nstate                object\ndtype: object\n\n\n\ndf['experience'].describe()\n\ncount    387.000000\nmean       4.680879\nstd        2.908509\nmin        1.000000\n25%        3.000000\n50%        4.000000\n75%        5.000000\nmax       20.000000\nName: experience, dtype: float64\n\n\n\ndf['salary'].describe()\n\ncount       332.000000\nmean     143013.671687\nstd       61496.830081\nmin       22070.000000\n25%      107925.000000\n50%      148887.500000\n75%      180412.500000\nmax      400000.000000\nName: salary, dtype: float64\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 823 entries, 0 to 822\nData columns (total 15 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Unnamed: 0        823 non-null    int64  \n 1   title             823 non-null    object \n 2   company_name      823 non-null    object \n 3   location          823 non-null    object \n 4   via               823 non-null    object \n 5   description       823 non-null    object \n 6   schedule_type     822 non-null    object \n 7   salary            332 non-null    float64\n 8   query             823 non-null    object \n 9   qualifications    823 non-null    object \n 10  responsibilities  718 non-null    object \n 11  benefits          423 non-null    object \n 12  degree            536 non-null    object \n 13  experience        387 non-null    float64\n 14  remote            823 non-null    bool   \ndtypes: bool(1), float64(2), int64(1), object(11)\nmemory usage: 90.9+ KB\n\n\n\n(df.isnull().sum()/(len(df)))*100 # percent of missing data\n\nUnnamed: 0           0.000000\ntitle                0.000000\ncompany_name         0.000000\nlocation             0.000000\nvia                  0.000000\ndescription          0.000000\nschedule_type        0.121507\nsalary              59.659781\nquery                0.000000\nqualifications       0.000000\nresponsibilities    12.758202\nbenefits            48.602673\ndegree              34.872418\nexperience          52.976914\nremote               0.000000\ndtype: float64\n\n\n\ncat_cols=df.select_dtypes(include=['object']).columns\nnum_cols = df.select_dtypes(include=np.number).columns.tolist()\nprint(\"Categorical Variables:\")\nprint(cat_cols)\nprint(\"Numerical Variables:\")\nprint(num_cols)\n\nCategorical Variables:\nIndex(['title', 'company_name', 'location', 'via', 'description',\n       'schedule_type', 'query', 'qualifications', 'responsibilities',\n       'benefits', 'degree'],\n      dtype='object')\nNumerical Variables:\n['Unnamed: 0', 'salary', 'experience']"
  },
  {
    "objectID": "data-exploration.html#univariate-and-bivariate-analysis",
    "href": "data-exploration.html#univariate-and-bivariate-analysis",
    "title": "Exploratory Data Analysis",
    "section": "3 Univariate and Bivariate Analysis",
    "text": "3 Univariate and Bivariate Analysis\nAnother important piece of EDA is to explore the raw data visually, either by means of Univariate or Bivariate analysis. Univariate analysis entails viewing the distribution and features of one variable. It helps us understand the distribution of a single variable, such as the frequency of each value or the range of values in our data. Bivariate analysis, on the other hand, relates to plotting the relationship between two variables. It helps us understand the relationship between two variables, such as correlation or association. Bivariate plots such as scatter plots, line plots, and heatmaps can be used to identify patterns, trends, and dependencies in the data.\n\n# create a histogram of the data\nplt.hist(df['salary'], bins=30, density=True, alpha=0.5, color='green')\n\nplt.title('Histogram of Salaries')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\n\nplt.show()\n\n\n\n\n\n# create a histogram of the data\nplt.hist(df['experience'], density=True, alpha=0.5, color='green')\n\nplt.title('Histogram of Experience')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\n\nplt.show()\n\n\n\n\n\nsns.boxplot(x='remote', y='salary', data=df)\n\nplt.title('Boxplot of Salary by Remote Work Status')\nplt.xlabel('Remote Work Status')\nplt.ylabel('Salary')\n\nplt.show()\n\n\n\n\n\nsns.boxplot(x='degree', y='salary', data=df)\n\nplt.title('Boxplot of Salary by Degree')\nplt.xlabel('Degree')\nplt.ylabel('Salary')\n\nplt.show()\n\n\n\n\n\nsns.boxplot(x='remote', y='experience', data=df)\n\nplt.title('Boxplot of Experience by Remote Work Status')\nplt.xlabel('Remote Work Status')\nplt.ylabel('Experience')\n\nplt.show()\n\n\n\n\n\nsns.scatterplot(x='experience', y='salary', hue = 'degree', data=df)\n\nplt.title('Correlational Plot of Experience and Salary')\nplt.xlabel('Experience')\nplt.ylabel('Salary')\n\nplt.show()\n\n\n\n\n\nsns.countplot(data = df, x = 'degree')\n\n<AxesSubplot: xlabel='degree', ylabel='count'>\n\n\n\n\n\n\nsns.countplot(data = df, x = 'remote')\n\n<AxesSubplot: xlabel='remote', ylabel='count'>\n\n\n\n\n\n\nsns.boxplot(data = df, x='query', y='salary')\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n [Text(0, 0, 'blockchain'),\n  Text(1, 0, 'natural language processing'),\n  Text(2, 0, 'big data and cloud computing'),\n  Text(3, 0, 'data analyst'),\n  Text(4, 0, 'machine learning'),\n  Text(5, 0, 'reinforcement learning'),\n  Text(6, 0, 'neural networks'),\n  Text(7, 0, 'deep learning'),\n  Text(8, 0, 'data scientist'),\n  Text(9, 0, 'time series'),\n  Text(10, 0, 'time series analysis')])\n\n\n\n\n\n\nsns.boxplot(data = df, x='query', y='experience')\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n [Text(0, 0, 'blockchain'),\n  Text(1, 0, 'natural language processing'),\n  Text(2, 0, 'big data and cloud computing'),\n  Text(3, 0, 'data analyst'),\n  Text(4, 0, 'machine learning'),\n  Text(5, 0, 'reinforcement learning'),\n  Text(6, 0, 'neural networks'),\n  Text(7, 0, 'deep learning'),\n  Text(8, 0, 'data scientist'),\n  Text(9, 0, 'time series'),\n  Text(10, 0, 'time series analysis')])\n\n\n\n\n\n\ndf['company_name'].value_counts().head(10)\n\nUpwork                 56\nBooz Allen Hamilton    17\nApple                  15\nDeloitte               10\nWalmart                 9\nJobot                   8\nSnap Inc.               8\nLockheed Martin         7\nMicrosoft               7\nLeidos                  7\nName: company_name, dtype: int64\n\n\n\ndf['via'].value_counts().head(10)\n\nLinkedIn          66\nZipRecruiter      60\nUpwork            56\nAngelList         36\nLever             23\nGreenhouse        19\nClearance Jobs    19\nStartup Jobs      16\nBuilt In          15\nSalary.com        14\nName: via, dtype: int64\n\n\n\n# get the top 10 most frequent job titles\ntop_5_titles = df['title'].value_counts().head(5)\n\n# create a bar plot of the job title counts\nsns.barplot(x=top_5_titles, y=top_5_titles.index)\nplt.title('Top 10 Job Titles')\nplt.xlabel('Count')\nplt.ylabel('Job Title')\nplt.show()\n\n\n\n\n\nsns.heatmap(df[['salary', 'experience']].corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\nsns.kdeplot(data=df, x='experience')\nplt.title('Distribution of Experience')\nplt.show()\n\n\n\n\n\n# create a bar plot of degree counts\nsns.countplot(data=df, x='degree')\nplt.title('Degree Counts')\nplt.show()\n\n\n\n\n\ntop_8_locations = df['location'].value_counts().head(8)\n\n# create a bar plot of the job title counts\nsns.barplot(x=top_8_locations, y=top_8_locations.index)\nplt.title('Top 8 Job Locations')\nplt.xlabel('Count')\nplt.ylabel('Job Location')\nplt.show()"
  }
]